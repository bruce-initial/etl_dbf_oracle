"""
Data Quality Checker for ETL pipeline.
Validates data transfer completeness and content accuracy.
"""

import logging
import polars as pl
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from ..database.operations import DatabaseOperations

logger = logging.getLogger(__name__)


class DataQualityChecker:
    """
    Performs data quality checks after ETL operations.
    
    Validates:
    1. Row count completeness between source and target
    2. Column content accuracy for a sample of rows
    """
    
    def __init__(self, db_operations: DatabaseOperations):
        """
        Initialize data quality checker.
        
        Args:
            db_operations: DatabaseOperations instance for Oracle access
        """
        self.db_operations = db_operations
        self.quality_table = "DATA_QUALITY_CHECKING_LEIS"
    
    def create_quality_table(self) -> None:
        """Create the data quality checking results table if it doesn't exist."""
        try:
            oracle_conn = self.db_operations.connection.get_connection()
            cursor = oracle_conn.cursor()
            
            # Check if table exists
            check_sql = """
                SELECT COUNT(*) FROM user_tables WHERE table_name = :table_name
            """
            cursor.execute(check_sql, [self.quality_table])
            table_exists = cursor.fetchone()[0] > 0
            
            if not table_exists:
                create_sql = f"""
                CREATE TABLE {self.quality_table} (
                    ID NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
                    TABLE_NAME VARCHAR2(128) NOT NULL,
                    CHECK_TYPE VARCHAR2(50) NOT NULL,
                    CHECK_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    SOURCE_COUNT NUMBER,
                    TARGET_COUNT NUMBER,
                    SAMPLE_SIZE NUMBER,
                    MATCHING_ROWS NUMBER,
                    ACCURACY_PERCENTAGE NUMBER(5,2),
                    STATUS VARCHAR2(20) NOT NULL,
                    ERROR_MESSAGE VARCHAR2(4000),
                    CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
                cursor.execute(create_sql)
                oracle_conn.commit()
                logger.info(f"Created data quality table: {self.quality_table}")
            else:
                logger.debug(f"Data quality table already exists: {self.quality_table}")
            
            cursor.close()
            
        except Exception as e:
            logger.error(f"Failed to create quality table: {e}")
            raise
    
    def check_row_count_completeness(self, source_df: pl.DataFrame, target_table: str,
                                    source_file_identifier: Optional[str] = None) -> Dict[str, Any]:
        """
        Compare row counts between source DataFrame and target Oracle table.
        
        Args:
            source_df: Source DataFrame
            target_table: Target Oracle table name
            source_file_identifier: Identifier for source file to filter target data
            
        Returns:
            Dictionary with completeness check results
        """
        check_result = {
            'check_type': 'ROW_COUNT',
            'table_name': target_table,
            'check_time': datetime.now(),
            'status': 'FAILED',
            'error_message': None
        }
        
        try:
            # Get source count
            source_count = len(source_df)
            check_result['source_count'] = source_count
            
            # Get target count, filtered by source file if specified
            if source_file_identifier:
                count_query = f"SELECT COUNT(*) FROM {target_table} WHERE SOURCE_FILE = '{source_file_identifier}'"
                logger.info(f"Row count check for {target_table} filtered by SOURCE_FILE = '{source_file_identifier}'")
            else:
                count_query = f"SELECT COUNT(*) FROM {target_table}"
                logger.info(f"Row count check for {target_table} (entire table)")
                
            target_df = self.db_operations.execute_query(count_query)
            target_count = target_df[0, 0]  # First row, first column
            check_result['target_count'] = target_count
            
            # Calculate completeness
            if source_count == target_count:
                check_result['status'] = 'PASSED'
                check_result['accuracy_percentage'] = 100.0
            else:
                check_result['status'] = 'FAILED'
                if source_count > 0:
                    check_result['accuracy_percentage'] = round((target_count / source_count) * 100, 2)
                else:
                    check_result['accuracy_percentage'] = 0.0
                
                missing_rows = source_count - target_count
                check_result['error_message'] = f"Missing {missing_rows} rows ({missing_rows/source_count*100:.2f}%)"
            
            logger.info(f"Row count check for {target_table}: {source_count} source â†’ {target_count} target ({check_result['accuracy_percentage']}%)")
            
        except Exception as e:
            check_result['error_message'] = str(e)
            logger.error(f"Row count check failed for {target_table}: {e}")
        
        return check_result
    
    def check_content_accuracy(self, source_df: pl.DataFrame, target_table: str, 
                              sample_percentage: float = 10.0, 
                              column_mapping: Optional[Dict[str, str]] = None,
                              exclude_columns: Optional[List[str]] = None,
                              source_file_identifier: Optional[str] = None) -> Dict[str, Any]:
        """
        Compare content between source DataFrame and target table for a sample of rows.
        For merged data from multiple source files, this method compares the source data
        against only the corresponding rows in the target table (filtered by source_file_identifier).
        
        Args:
            source_df: Source DataFrame (data from a single file or merged data)
            target_table: Target Oracle table name
            sample_percentage: Percentage of rows to sample for content validation
            column_mapping: Optional mapping from source to target column names
            exclude_columns: List of column names to exclude from comparison (e.g., ['U_ID', 'SOURCE_FILE'])
            source_file_identifier: Identifier for source file to filter target data (for composite tables)
            
        Returns:
            Dictionary with content accuracy check results
        """
        check_result = {
            'check_type': 'CONTENT_ACCURACY',
            'table_name': target_table,
            'check_time': datetime.now(),
            'status': 'FAILED',
            'error_message': None
        }
        
        try:
            source_count = len(source_df)
            if source_count == 0:
                check_result['error_message'] = "Source DataFrame is empty"
                return check_result
            
            # Set default exclude columns if not provided
            if exclude_columns is None:
                exclude_columns = ['U_ID', 'SOURCE_FILE']  # Default system-generated columns
            
            # Prepare source data for comparison
            comparison_source_df = source_df
            
            # Apply column mapping to source if provided
            if column_mapping:
                comparison_source_df = comparison_source_df.rename(column_mapping)
            
            # Get target data - if source_file_identifier is provided, filter by SOURCE_FILE
            base_query = f"SELECT * FROM {target_table}"
            where_conditions = []
            
            if source_file_identifier and 'SOURCE_FILE' not in exclude_columns:
                where_conditions.append(f"SOURCE_FILE = '{source_file_identifier}'")
            
            where_clause = ""
            if where_conditions:
                where_clause = "WHERE " + " AND ".join(where_conditions)
            
            target_query = f"{base_query} {where_clause}".strip()
            target_df = self.db_operations.execute_query(target_query)
            
            if len(target_df) == 0:
                check_result['error_message'] = f"Target table is empty{' for source file ' + source_file_identifier if source_file_identifier else ''}"
                return check_result
            
            # Align columns between source and target, excluding system-generated columns
            target_columns_upper = [tc.upper() for tc in target_df.columns]
            exclude_columns_upper = [col.upper() for col in exclude_columns]
            
            # Find common columns but exclude system-generated ones
            common_columns = []
            source_col_mapping = {}  # Map from original column name to target column name
            
            for col in comparison_source_df.columns:
                col_upper = col.upper()
                if col_upper in target_columns_upper and col_upper not in exclude_columns_upper:
                    # Find the exact target column name (preserving case)
                    target_col_name = next(tc for tc in target_df.columns if tc.upper() == col_upper)
                    common_columns.append(col)
                    source_col_mapping[col] = target_col_name
            
            if not common_columns:
                excluded_info = f"Excluded columns: {exclude_columns}" if exclude_columns else "No exclusions"
                check_result['error_message'] = f"No common columns found between source and target after exclusions. {excluded_info}"
                return check_result
            
            logger.info(f"Content accuracy check will compare {len(common_columns)} columns: {common_columns}")
            logger.info(f"Excluding columns: {exclude_columns}")
            
            # For content comparison, we need to handle the fact that data might be ordered differently
            # Calculate sample size based on the smaller dataset
            sample_size = max(1, int((sample_percentage / 100) * min(source_count, len(target_df))))
            check_result['sample_size'] = sample_size
            
            # Select only the common columns for comparison
            source_comparison_df = comparison_source_df.select(common_columns)
            target_comparison_df = target_df.select([source_col_mapping[col] for col in common_columns])
            
            # Rename target columns to match source for easier comparison
            target_comparison_df = target_comparison_df.rename({source_col_mapping[col]: col for col in common_columns})
            
            # Sample rows from source data (use seed for reproducibility)
            if sample_size >= source_count:
                sampled_source = source_comparison_df
            else:
                sampled_source = source_comparison_df.sample(n=sample_size, seed=42)
            
            # Convert both DataFrames to string representation for robust comparison
            # This handles different data types and null values consistently
            sampled_source_str = sampled_source.with_columns([
                pl.col(col).cast(pl.Utf8).fill_null("").str.strip_chars().alias(col) 
                for col in common_columns
            ])
            
            target_comparison_str = target_comparison_df.with_columns([
                pl.col(col).cast(pl.Utf8).fill_null("").str.strip_chars().alias(col) 
                for col in common_columns
            ])
            
            # Compare data by checking if each source row exists in target data
            matching_rows = 0
            mismatch_details = []
            max_mismatches_to_log = 5
            
            source_rows_data = sampled_source_str.to_dicts()
            target_rows_data = target_comparison_str.to_dicts()
            
            # Create a set of target rows for faster lookup
            target_rows_set = set()
            for target_row in target_rows_data:
                # Create a hashable tuple from the row values
                row_tuple = tuple(target_row[col] for col in common_columns)
                target_rows_set.add(row_tuple)
            
            for i, source_row in enumerate(source_rows_data):
                # Create comparable tuple from source row
                source_row_tuple = tuple(source_row[col] for col in common_columns)
                
                if source_row_tuple in target_rows_set:
                    matching_rows += 1
                else:
                    # This source row was not found in target - collect mismatch details
                    if len(mismatch_details) < max_mismatches_to_log:
                        # Find the most similar target row for detailed comparison
                        # Calculate similarity scores for all target rows
                        best_match_row = None
                        best_match_score = -1
                        
                        for target_row in target_rows_data:
                            # Calculate how many columns match between source and this target row
                            match_count = sum(1 for col in common_columns if source_row[col] == target_row[col])
                            if match_count > best_match_score:
                                best_match_score = match_count
                                best_match_row = target_row
                        
                        # Show detailed comparison with the best matching target row (or first row if no good match)
                        comparison_target_row = best_match_row if best_match_row else target_rows_data[0] if target_rows_data else {}
                        
                        row_mismatches = []
                        for col in common_columns:
                            source_val = source_row[col]
                            target_val = comparison_target_row.get(col, "NO_TARGET_DATA") if comparison_target_row else "NO_TARGET_DATA"
                            
                            # For mismatch details, show what we expected vs what we found
                            row_mismatches.append({
                                'column': col,
                                'source_value': source_val[:100] if source_val else "",
                                'target_value': target_val[:100] if target_val and target_val != "NO_TARGET_DATA" else target_val
                            })
                        
                        mismatch_details.append({
                            'row_index': i + 1,
                            'mismatches': row_mismatches,
                            'match_info': f"Best target match had {best_match_score}/{len(common_columns)} columns matching" if best_match_row else "No target data available"
                        })
            
            check_result['matching_rows'] = matching_rows
            check_result['sample_size'] = len(source_rows_data)
            
            # Calculate accuracy percentage
            if len(source_rows_data) > 0:
                accuracy = (matching_rows / len(source_rows_data)) * 100
                check_result['accuracy_percentage'] = round(accuracy, 2)
                
                if accuracy == 100.0:
                    check_result['status'] = 'PASSED'
                else:
                    check_result['status'] = 'FAILED'
                    mismatched_rows = len(source_rows_data) - matching_rows
                    
                    # Build detailed error message with mismatch information
                    error_parts = [f"Content mismatch in {mismatched_rows}/{len(source_rows_data)} sampled rows."]
                    error_parts.append(f"Source data: {source_count} rows, Target data: {len(target_df)} rows.")
                    
                    if source_file_identifier:
                        error_parts.append(f"Target filtered by SOURCE_FILE = '{source_file_identifier}'")
                    
                    if mismatch_details:
                        error_parts.append("Mismatch details (first few rows):")
                        for detail in mismatch_details:
                            row_info = f"Row {detail['row_index']}:"
                            column_comparisons = []
                            
                            # Show detailed source vs target comparison for each column
                            for mismatch in detail['mismatches'][:3]:  # Limit columns shown per row
                                col_name = mismatch['column']
                                src_val = mismatch['source_value']
                                tgt_val = mismatch['target_value']
                                
                                # Format the comparison clearly
                                if tgt_val == "NO_TARGET_DATA":
                                    col_info = f"{col_name}(src:'{src_val}' -> NO_TARGET_DATA)"
                                else:
                                    col_info = f"{col_name}(src:'{src_val}' vs tgt:'{tgt_val}')"
                                column_comparisons.append(col_info)
                            
                            # Add match info if available
                            match_info = detail.get('match_info', '')
                            row_details = f"{row_info} {', '.join(column_comparisons)}"
                            if match_info:
                                row_details += f" [{match_info}]"
                            
                            error_parts.append(row_details)
                        
                        # Add summary if there are more mismatches than logged
                        if mismatched_rows > len(mismatch_details):
                            additional_mismatches = mismatched_rows - len(mismatch_details)
                            error_parts.append(f"... and {additional_mismatches} more rows with mismatches")
                        
                        # Add comprehensive data discrepancy information
                        error_parts.append("=== SOURCE vs TARGET DATA DISCREPANCIES ===")
                        
                        # Add source data sample
                        if source_rows_data and len(source_rows_data) > 0:
                            error_parts.append(f"SOURCE data sample (first {min(3, len(source_rows_data))} rows):")
                            for idx, source_row in enumerate(source_rows_data[:3]):
                                source_row_values = []
                                for col in common_columns[:3]:  # Limit columns to avoid too much data
                                    val = source_row.get(col, 'NULL')
                                    val_str = str(val)
                                    truncated_val = val_str[:50] + ('...' if len(val_str) > 50 else '')
                                    source_row_values.append(f"{col}='{truncated_val}'")
                                error_parts.append(f"  Source[{idx+1}]: {', '.join(source_row_values)}")
                            
                            if len(source_rows_data) > 3:
                                error_parts.append(f"  ... and {len(source_rows_data) - 3} more source rows")
                        
                        # Add target data sample  
                        if target_rows_data and len(target_rows_data) > 0:
                            error_parts.append(f"TARGET data sample (first {min(3, len(target_rows_data))} rows):")
                            for idx, target_row in enumerate(target_rows_data[:3]):
                                target_row_values = []
                                for col in common_columns[:3]:  # Limit columns to avoid too much data
                                    val = target_row.get(col, 'NULL')
                                    val_str = str(val)
                                    truncated_val = val_str[:50] + ('...' if len(val_str) > 50 else '')
                                    target_row_values.append(f"{col}='{truncated_val}'")
                                error_parts.append(f"  Target[{idx+1}]: {', '.join(target_row_values)}")
                            
                            if len(target_rows_data) > 3:
                                error_parts.append(f"  ... and {len(target_rows_data) - 3} more target rows")
                        
                        # Add column-by-column data analysis
                        error_parts.append("COLUMN ANALYSIS:")
                        for col in common_columns[:5]:  # Analyze first 5 columns
                            source_values = [row.get(col, 'NULL') for row in source_rows_data[:10]]
                            target_values = [row.get(col, 'NULL') for row in target_rows_data[:10]]
                            
                            source_unique = set(str(v) for v in source_values if v is not None)
                            target_unique = set(str(v) for v in target_values if v is not None)
                            
                            error_parts.append(f"  {col}: Source_unique={len(source_unique)} Target_unique={len(target_unique)}")
                            
                            # Show unique values if sets are small
                            if len(source_unique) <= 5 and len(target_unique) <= 5:
                                error_parts.append(f"    Source_vals: {sorted(list(source_unique))}")
                                error_parts.append(f"    Target_vals: {sorted(list(target_unique))}")
                            
                            # Show intersection and differences
                            common_vals = source_unique & target_unique
                            source_only = source_unique - target_unique
                            target_only = target_unique - source_unique
                            
                            if common_vals:
                                error_parts.append(f"    Common: {len(common_vals)} values")
                            if source_only:
                                error_parts.append(f"    Source_only: {list(source_only)[:3]}")
                            if target_only:
                                error_parts.append(f"    Target_only: {list(target_only)[:3]}")
                        
                        error_parts.append("=== END DISCREPANCY ANALYSIS ===")
                    
                    # Combine all error parts, but ensure we don't exceed database field limits
                    full_error_message = " ".join(error_parts)
                    if len(full_error_message) > 3900:  # Leave room for Oracle VARCHAR2(4000) limit
                        # Truncate and add indicator
                        check_result['error_message'] = full_error_message[:3900] + "... (truncated)"
                    else:
                        check_result['error_message'] = full_error_message
            else:
                check_result['accuracy_percentage'] = 0.0
                check_result['error_message'] = "No rows could be compared"
            
            logger.info(f"Content accuracy check for {target_table}: {matching_rows}/{len(source_rows_data)} matching rows ({check_result['accuracy_percentage']}%)")
            
        except Exception as e:
            check_result['error_message'] = str(e)
            logger.error(f"Content accuracy check failed for {target_table}: {e}")
        
        return check_result
    
    def perform_quality_checks(self, source_df: pl.DataFrame, target_table: str, 
                              sample_percentage: float = 10.0, 
                              column_mapping: Optional[Dict[str, str]] = None,
                              exclude_columns: Optional[List[str]] = None,
                              source_file_identifier: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        Perform both row count and content accuracy checks.
        
        Args:
            source_df: Source DataFrame
            target_table: Target Oracle table name
            sample_percentage: Percentage of rows to sample for content validation
            column_mapping: Optional mapping from source to target column names
            exclude_columns: List of column names to exclude from comparison
            source_file_identifier: Identifier for source file to filter target data
            
        Returns:
            List of check results
        """
        results = []
        
        try:
            # Ensure quality table exists
            self.create_quality_table()
            
            # Perform row count check
            logger.info(f"Performing row count check for {target_table}")
            row_count_result = self.check_row_count_completeness(source_df, target_table, source_file_identifier)
            results.append(row_count_result)
            
            # Perform content accuracy check
            logger.info(f"Performing content accuracy check for {target_table} (sample: {sample_percentage}%)")
            content_result = self.check_content_accuracy(
                source_df, target_table, sample_percentage, column_mapping, 
                exclude_columns, source_file_identifier
            )
            results.append(content_result)
            
            # Store results in quality table
            self._store_quality_results(results)
            
        except Exception as e:
            logger.error(f"Quality checks failed for {target_table}: {e}")
            error_result = {
                'check_type': 'SYSTEM_ERROR',
                'table_name': target_table,
                'check_time': datetime.now(),
                'status': 'FAILED',
                'error_message': str(e)
            }
            results.append(error_result)
        
        return results
    
    def _store_quality_results(self, results: List[Dict[str, Any]]) -> None:
        """
        Store quality check results in the quality table.
        
        Args:
            results: List of quality check results
        """
        try:
            oracle_conn = self.db_operations.connection.get_connection()
            cursor = oracle_conn.cursor()
            
            insert_sql = f"""
                INSERT INTO {self.quality_table} 
                (TABLE_NAME, CHECK_TYPE, CHECK_TIME, SOURCE_COUNT, TARGET_COUNT, 
                 SAMPLE_SIZE, MATCHING_ROWS, ACCURACY_PERCENTAGE, STATUS, ERROR_MESSAGE)
                VALUES (:1, :2, :3, :4, :5, :6, :7, :8, :9, :10)
            """
            
            for result in results:
                params = [
                    result.get('table_name'),
                    result.get('check_type'),
                    result.get('check_time'),
                    result.get('source_count'),
                    result.get('target_count'),
                    result.get('sample_size'),
                    result.get('matching_rows'),
                    result.get('accuracy_percentage'),
                    result.get('status'),
                    result.get('error_message')
                ]
                cursor.execute(insert_sql, params)
            
            oracle_conn.commit()
            cursor.close()
            
            logger.info(f"Stored {len(results)} quality check results in {self.quality_table}")
            
        except Exception as e:
            logger.error(f"Failed to store quality results: {e}")
            # Don't raise here as the ETL should not fail just because we can't store quality results
    
    def get_quality_report(self, table_name: Optional[str] = None) -> pl.DataFrame:
        """
        Retrieve quality check results from the database.
        
        Args:
            table_name: Optional table name to filter results
            
        Returns:
            DataFrame with quality check results
        """
        try:
            base_query = f"""
                SELECT 
                    ID,
                    TABLE_NAME,
                    CHECK_TYPE,
                    CHECK_TIME,
                    SOURCE_COUNT,
                    TARGET_COUNT,
                    SAMPLE_SIZE,
                    MATCHING_ROWS,
                    ACCURACY_PERCENTAGE,
                    STATUS,
                    ERROR_MESSAGE,
                    CREATED_AT
                FROM {self.quality_table}
            """
            
            if table_name:
                query = base_query + " WHERE TABLE_NAME = :table_name ORDER BY CHECK_TIME DESC"
                # For Oracle, we need to pass parameters as a list, not dict
                # But execute_query doesn't support parameters, so we'll use string formatting (safely)
                # In production, you should use proper parameter binding
                query = base_query + f" WHERE TABLE_NAME = '{table_name}' ORDER BY CHECK_TIME DESC"
            else:
                query = base_query + " ORDER BY CHECK_TIME DESC"
            
            return self.db_operations.execute_query(query)
            
        except Exception as e:
            logger.error(f"Failed to retrieve quality report: {e}")
            # Return empty DataFrame with expected structure
            return pl.DataFrame({
                'ID': [],
                'TABLE_NAME': [],
                'CHECK_TYPE': [],
                'CHECK_TIME': [],
                'SOURCE_COUNT': [],
                'TARGET_COUNT': [],
                'SAMPLE_SIZE': [],
                'MATCHING_ROWS': [],
                'ACCURACY_PERCENTAGE': [],
                'STATUS': [],
                'ERROR_MESSAGE': [],
                'CREATED_AT': []
            })